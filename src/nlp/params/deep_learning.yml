use_gpu: true

training:
  model_name: MilaNLProc/hate-ita

  eval_size: 0.25
  model_max_length: 256 # max is 512 for XMLRoberta, so it depends on the backbone model

  batch_size: 8
  test_batch_size: 16
  learning_rate: 5.0e-5
  decay: 0.0
  freeze_base: true
  epochs: 3

  resume: false
  checkpoint: dumps/nlp_models/MilaNLProc_hate-ita_5 # path to folder with checkpoints (last epoch will be loaded)

testing:
  task_m_model_name: dumps/nlp_models/MilaNLProc_hate-ita_2/checkpoint-160 # "Hate-speech-CNERG/dehatebert-mono-italian OR path to existing checkpoint
  task_a_model_name: dumps/nlp_models/MilaNLProc_hate-ita_2/checkpoint-160 # "Hate-speech-CNERG/dehatebert-mono-italian OR path to existing checkpoint
  target_label: hateful # target for inference, depends on pre-trained model